{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs a temporal splitting bootstrap. We import assay date and train on 70% of the earlier data - test on the remaining third.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import joblib\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [s.replace('datasets/', '') for s in glob('datasets/*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import AllChem\n",
    "\n",
    "data_dir = 'appendix/data/'\n",
    "\n",
    "def make_data(tgt):\n",
    "    \n",
    "    ntgt = tgt\n",
    "    if ntgt == 'JAK2':\n",
    "        ntgt = 'JAK-2'\n",
    "    fn = data_dir + ntgt + '/' + ntgt + '.sdf'\n",
    "    \n",
    "    dy_dict = {}\n",
    "    \n",
    "    unk = 2100 # way off into the future\n",
    "    \n",
    "    suppl = Chem.SDMolSupplier(fn)\n",
    "    for mol in suppl:\n",
    "        smi = Chem.MolToSmiles(mol)\n",
    "        try:\n",
    "            dy_dict[smi] = int(mol.GetProp('document_year'))\n",
    "        except ValueError:\n",
    "            dy_dict[smi] = unk\n",
    "        pass\n",
    "    \n",
    "    sms = joblib.load('datasets/' + tgt + '/' + tgt + '.smiu')\n",
    "    \n",
    "    dy_out = np.asarray([dy_dict.get(x, unk) for x in sms])\n",
    "    \n",
    "    joblib.dump(dy_out, 'datasets/' + tgt + '/' + tgt + '.dyu')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt in targets:\n",
    "    make_data(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from scipy.spatial.distance import pdist, squareform, jaccard, cityblock\n",
    "from scipy import stats\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import linear_model as LM\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, BayesianRidge, ElasticNet, Lasso\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Deep learning model with intermediate layer...\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=128, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=100, batch_size=5, verbose=0)))\n",
    "\n",
    "# Model dictionaries \n",
    "model_dict = {'ridge' : {'m' : Ridge, 'kw' : {'fit_intercept':True, 'alpha':0.1}, },\n",
    "              'rf'    : {'m' : RandomForestRegressor, 'kw' : {'n_estimators':100, \n",
    "                                                              'n_jobs':4, 'max_depth':10}, },\n",
    "              'svr'   : {'m' : SVR, 'kw' : {}, },\n",
    "              \n",
    "              'dl_l'   : {'m' : Pipeline, \n",
    "                          'kw' : {'steps': [('standardize', StandardScaler()),\n",
    "                                            ('mlp', KerasRegressor(build_fn=larger_model, \n",
    "                                                                   epochs=100, batch_size=5, \n",
    "                                                                   verbose=0))\n",
    "                                                           ]},\n",
    "                         },\n",
    "             }\n",
    "\n",
    "# Datasets\n",
    "from glob import glob\n",
    "targets = [s.replace('datasets/', '') for s in glob('datasets/*')]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "figsize(20, 10)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs without deep learning are much faster - this controls use of DL\n",
    "do_dl = True\n",
    "\n",
    "# Specify whether to run the models\n",
    "force_rerun = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for overall number of runs\n",
    "if not do_dl:\n",
    "    # Take out when we want to run deep learning model\n",
    "    del model_dict['dl_l']\n",
    "    tot_num_runs = 400\n",
    "    outdir = 'models_final/'\n",
    "    fig_dir = 'figures/'\n",
    "    ['svr', 'rf', 'ridge']\n",
    "    model_labels = ['Support Vector Regression','Random Forests','Ridge Regression']\n",
    "else:\n",
    "    tot_num_runs = 400\n",
    "    outdir = 'models_final_dl/'\n",
    "    fig_dir = 'figures_dl/'\n",
    "    model_labels = ['Support Vector Regression','Random Forests','Ridge Regression','Deep Learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data for a single target name\n",
    "def get_data(tgt_name='COX-2'):\n",
    "    data_dir = 'datasets/' + tgt_name + '/'\n",
    "    preds = joblib.load(data_dir + tgt_name + '_predsu.npy')\n",
    "    resps = joblib.load(data_dir + tgt_name + '_respu.npy')\n",
    "    smiles = joblib.load(data_dir + tgt_name + '.smiu')\n",
    "    dy = joblib.load(data_dir + tgt_name + '.dyu')\n",
    "    return preds, resps, smiles, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fn(frac_fit, kf=0, insample=False, fig=False, loss=None, dy=False):\n",
    "    fn = 'loss_' + str(frac_fit)\n",
    "    if kf > 0:\n",
    "        fn += '_kf_' + str(kf)\n",
    "        pass\n",
    "    if insample:\n",
    "        fn += '_insample'\n",
    "        pass\n",
    "    if dy:\n",
    "        fn += '_dy'\n",
    "    od = fig_dir if fig else outdir\n",
    "    ffn = os.path.join(od, fn)\n",
    "    if fig:\n",
    "        ffn = re.sub('[.]','', ffn)\n",
    "        ffn = ffn + '_' + loss + '.pdf'\n",
    "        pass\n",
    "    return ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_bootstrap(preds, resps, method, num_runs=100, insample=False,\n",
    "                  frac_fit=1.0, num_kf=0, use_pool=True, dy=None, single_sort=False):\n",
    "\n",
    "    # losses is going to be a list of dicts, loss_type => value\n",
    "    losses = []\n",
    "    \n",
    "    # You're doing time splitting...\n",
    "    if dy is not None:\n",
    "        sorted_indices = np.argsort(dy)\n",
    "    else:\n",
    "        sorted_indices = np.argsort(resps)\n",
    "        pass\n",
    "    \n",
    "    N = len(resps)\n",
    "    M = int(N * frac_fit)\n",
    "    \n",
    "    # Choose insample values...\n",
    "    idx_list = []\n",
    "    for i in range(num_runs):\n",
    "        # You're doing a single run on the sorted points\n",
    "        if single_sort:\n",
    "            idcs = sorted_indices[:M]\n",
    "        # You're doing cross-validation\n",
    "        elif num_kf > 0:\n",
    "            kf = KFold(n_splits=num_kf, shuffle=True)\n",
    "            for (tr_i, tst_i) in kf.split(np.arange(M)):\n",
    "                idcs = sorted_indices[tr_i]\n",
    "                pass\n",
    "            pass\n",
    "        # You're bootstrapping\n",
    "        else:\n",
    "            idcs_rand = np.random.choice(M, M)\n",
    "            idcs = sorted_indices[idcs_rand]\n",
    "            pass\n",
    "        idx_list.append({'is' : idcs})\n",
    "        pass\n",
    "    \n",
    "    # Add in all the other data...\n",
    "    for d in idx_list:\n",
    "        if insample:\n",
    "            oos = d['is']\n",
    "        else:\n",
    "            oos = np.delete(np.arange(N), d['is'])\n",
    "            pass\n",
    "        d.update({'oos' : oos, 'preds' : preds, \n",
    "                  'resps' : resps, 'method' : method})\n",
    "        pass\n",
    "            \n",
    "    if use_pool:\n",
    "        p = Pool(7)\n",
    "        losses = p.map(nbs_run, idx_list)\n",
    "        p.close()\n",
    "        p.join()\n",
    "    else:\n",
    "        losses = [nbs_run(x) for x in idx_list]\n",
    "        pass\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(frac_fit=1.0, kf=0, insample=False, use_pool=True, dy_split=False):\n",
    "\n",
    "    fnf = get_fn(frac_fit, kf=kf, insample=insample, dy=dy_split)\n",
    "    if os.path.isfile(fnf) and not force_rerun:\n",
    "        print('Already computed')\n",
    "        return\n",
    "\n",
    "    loss_hdr = {}\n",
    "    if insample:\n",
    "        num_runs = 1\n",
    "    elif kf > 0:\n",
    "        num_runs = int(tot_num_runs/kf)\n",
    "    else:\n",
    "        num_runs = tot_num_runs\n",
    "    \n",
    "    single_sort=False\n",
    "    # just run single fit\n",
    "    if dy_split:\n",
    "        num_runs = 1\n",
    "        single_sort=True\n",
    "    \n",
    "    for tgt in sorted_targets:\n",
    "        loss_hdr[tgt] = {}\n",
    "        print ('Doing', tgt)\n",
    "        preds, resps, _, dy_data = get_data(tgt)\n",
    "        dy=None if not dy_split else dy_data\n",
    "        \n",
    "        preds = preds + 0.\n",
    "        for m in model_dict.keys():\n",
    "            res = full_bootstrap(preds, resps, m, frac_fit=frac_fit, num_kf=kf,\n",
    "                                 insample=insample, num_runs=num_runs, use_pool=use_pool,\n",
    "                                 dy=dy, single_sort=single_sort,\n",
    "                                )\n",
    "            detail_fn = outdir + 'detail/' + tgt + '_' + m + '_' + str(frac_fit) + '_' + str(kf) \n",
    "            if insample:\n",
    "                detail_fn += '_insample'\n",
    "            if dy_split:\n",
    "                detail_fn += '_dy'\n",
    "            detail_fn += '.res'\n",
    "            \n",
    "            joblib.dump(res, detail_fn)\n",
    "            loss_hdr[tgt][m] = jackknife_summary(res)\n",
    "        pass\n",
    "    joblib.dump(loss_hdr, fnf)\n",
    "    print('Completed')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_mse(predictions, responses, **kwargs):\n",
    "    return mean_squared_error(responses, predictions) / mean_squared_error(responses, np.zeros_like(responses))\n",
    "\n",
    "\n",
    "def Rank_loss(predictions, responses, **kwargs):\n",
    "    \n",
    "    tgt_val = kwargs.get('tgt_val')\n",
    "    ranked = np.argsort(-predictions)\n",
    "    found = responses[ranked] >= tgt_val\n",
    "    # Number of actives\n",
    "    N_gamma = np.sum(found)\n",
    "    \n",
    "    # Size of test sets\n",
    "    N_test = predictions.shape[0]\n",
    "    lt = kwargs.get('loss_type')\n",
    "    #pdb.set_trace()\n",
    "    if lt == 'min':\n",
    "        # Equation (1) of the paper\n",
    "        loss = 1/(N_test - N_gamma) * np.min(np.arange(N_test)[found])\n",
    "    elif lt == 'avg':\n",
    "        # Equation (2) of the paper\n",
    "        loss = 1/N_gamma * 1/(N_test - N_gamma) * (np.sum(np.arange(N_test)[found]) - N_gamma * (N_gamma - 1)/2)\n",
    "        pass\n",
    "    \n",
    "    assert loss >= 0\n",
    "    assert loss <= 1\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Active_rank_loss_avg(predictions, responses, **kwargs):\n",
    "    kwargs.update({'loss_type' :'avg'})\n",
    "    return Rank_loss(predictions, responses, **kwargs)\n",
    "\n",
    "def Active_rank_loss_min(predictions, responses, **kwargs):\n",
    "    kwargs.update({'loss_type' :'min'})\n",
    "    return Rank_loss(predictions, responses, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "\n",
    "# define the dictionary of losses used here\n",
    "loss_dict = {'mse' :       {'func' : avg_mse,         'kw' : {}},\n",
    "             'loss_min_90' :   {'func' : Active_rank_loss_min,  'kw' : {'frac_find' : 0.9}},\n",
    "             'loss_avg_90' :   {'func' : Active_rank_loss_avg,  'kw' : {'frac_find' : 0.9}},\n",
    "             'loss_min_95' :   {'func' : Active_rank_loss_min,  'kw' : {'frac_find' : 0.95}},\n",
    "             'loss_avg_95' :   {'func' : Active_rank_loss_avg,  'kw' : {'frac_find' : 0.95}},\n",
    "             'loss_min_99' :   {'func' : Active_rank_loss_min,  'kw' : {'frac_find' : 0.99}},\n",
    "             'loss_avg_99' :   {'func' : Active_rank_loss_avg,  'kw' : {'frac_find' : 0.99}},\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_targets = sorted(targets, key=lambda x: len(get_data(x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbs_run(kwargs):\n",
    "    my_is = kwargs.get('is')\n",
    "    my_oos = kwargs.get('oos')\n",
    "    method = kwargs.get('method')\n",
    "    preds = kwargs.get('preds') + 0.\n",
    "    resps = kwargs.get('resps')\n",
    "    \n",
    "    mdl = model_dict[method]['m'](**model_dict[method]['kw'])\n",
    "    mdl.fit(preds[my_is], resps[my_is])\n",
    "        \n",
    "    predictions = mdl.predict(preds[my_oos])\n",
    "    \n",
    "    losses = {}\n",
    "    for (l, v) in loss_dict.iteritems():\n",
    "        \n",
    "        # Worth noting, we're looking for the top frac_find _in_the_oos_data_\n",
    "        # (not in the whole data - as otherwise we might be looking for something\n",
    "        # that isn't there)\n",
    "        if 'frac_find' in v['kw']:\n",
    "            N = len(resps[my_oos])\n",
    "            sorted_indices = np.argsort(resps[my_oos])\n",
    "            n = int(N * v['kw']['frac_find'])\n",
    "            tgt_val = resps[my_oos][sorted_indices[n]]\n",
    "            v['kw'].update({'tgt_val' : tgt_val})\n",
    "            \n",
    "        losses[l] = v['func'](predictions, resps[my_oos], **v['kw'])\n",
    "        pass\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the vector of observed values and their mean (implies fewer computations of the mean)\n",
    "def jackknife(vals, vals_bar):\n",
    "    n = len(vals)\n",
    "    var_JK = 0.0\n",
    "    for v in vals:\n",
    "        vals_bar_i = (n/(n-1)) * (vals_bar - v/n)\n",
    "        #print(pow(vals_bar_i - vals_bar, 2))\n",
    "        var_JK += pow(vals_bar_i - vals_bar, 2)\n",
    "        pass\n",
    "\n",
    "    sd_JK = pow( ((n-1)/n) * var_JK, .5)\n",
    "   \n",
    "    return sd_JK\n",
    "    \n",
    "def jackknife_summary(results):\n",
    "    summary = {}\n",
    "    for l in loss_dict.keys():\n",
    "        vals = np.asarray([x[l] for x in results])\n",
    "        if len(vals) == 1:\n",
    "            summary[l] = {'loss' : vals[0]}\n",
    "        else:\n",
    "            vals_bar = np.mean(vals)\n",
    "            sd = jackknife(vals, vals_bar)\n",
    "            summary[l] = {'loss_l' : vals_bar - 2*sd,\n",
    "                          'loss' : vals_bar,                          \n",
    "                          'loss_u' : vals_bar + 2*sd,\n",
    "                         }\n",
    "            pass\n",
    "        pass\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing A2a\n",
      "Doing Dopamine\n",
      "Doing Dihydrofolate\n",
      "Doing Carbonic\n",
      "Doing ABL1\n",
      "Doing opioid\n",
      "Doing Cannabinoid\n",
      "Doing Androgen\n",
      "Doing COX-1\n",
      "Doing Monoamine\n",
      "Doing LCK\n",
      "Doing Glucocortic\n",
      "Doing Ephrin\n",
      "Doing Caspase\n",
      "Doing Coagulation\n",
      "Doing Estrogen\n",
      "Doing B-raf\n",
      "Doing Glycogen\n"
     ]
    }
   ],
   "source": [
    "runner(frac_fit=0.7, use_pool=False, dy_split=True)\n",
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots results\n",
    "# Added put_title arg for plots for paper\n",
    "def plotter(frac_fit=1.0, kf=0, insample=False, loss='mse', save=True, put_title=False, \n",
    "            put_grid=False, dy_split=False):\n",
    "    \n",
    "    fnf = get_fn(frac_fit, kf=kf, insample=insample, dy=dy_split)\n",
    "    \n",
    "    loss_hdr = joblib.load(fnf)\n",
    "   \n",
    "    for (i, method) in enumerate(model_dict.keys()):\n",
    "\n",
    "        #losses = np.asarray([loss_hdr[x][method][loss]['loss'] for x in sorted_targets])\n",
    "        losses = []\n",
    "        for x in sorted_targets:\n",
    "            try:\n",
    "                losses.append(loss_hdr[x][method][loss]['loss'])\n",
    "            except:\n",
    "                print(x, method, loss)\n",
    "                raise KeyError\n",
    "        losses = np.asarray(losses)\n",
    "        if insample:\n",
    "            plot(np.arange(25) + 0.05*i, losses,  label=method.upper())\n",
    "        else:\n",
    "            loss_l = np.asarray([loss_hdr[x][method][loss]['loss_l'] for x in sorted_targets])\n",
    "            loss_u = np.asarray([loss_hdr[x][method][loss]['loss_u'] for x in sorted_targets])\n",
    "            \n",
    "            yerr = np.vstack((losses - loss_l, loss_u - losses))\n",
    "            \n",
    "            errorbar(np.arange(25)+i*0.1, losses, capsize=10, yerr=yerr, label=method.upper())\n",
    "            pass\n",
    "        pass\n",
    "        \n",
    "    if insample:\n",
    "        if put_title:\n",
    "            title('Insample %s Loss' % loss.upper())\n",
    "    else:\n",
    "        ttl = 'OOS %s Loss' % loss.upper()\n",
    "        if kf > 0:\n",
    "            ttl += ' with %d fold CV' % kf\n",
    "        if frac_fit < 1.0:\n",
    "            if dy_split:\n",
    "                'Split on discovery year at %.1f fraction'\n",
    "            else:\n",
    "                ttl += ' Max activity in fit at %.1f' % frac_fit\n",
    "        if put_title:\n",
    "            title(ttl)\n",
    "    pass\n",
    "\n",
    "    grid(put_grid)\n",
    "    plt.tick_params(top=False, right=False)\n",
    "    plt.xticks(np.arange(25), sorted_targets, rotation=-45)\n",
    "    plt.legend(loc='best', fontsize = 'x-large', labels=model_labels)\n",
    "    if loss=='mse':\n",
    "        plt.ylabel('Expected mean squared error', fontsize='x-large')\n",
    "    else:\n",
    "        plt.ylabel('Expected loss', fontsize='x-large')\n",
    "    \n",
    "    if save:\n",
    "        fnff = get_fn(frac_fit, kf=kf, insample=insample, fig=True, loss=loss)\n",
    "        print(fnff)\n",
    "        savefig(fnff, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnf = get_fn(0.7, kf=0, insample=False, dy=True)\n",
    "fnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.7, dy_split=True, put_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.7, dy_split=True, put_title=True, loss='loss_avg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.7, dy_split=True, put_title=True, loss='loss_avg_95')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.7, dy_split=True, put_title=True, loss='loss_min_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.7, dy_split=True, put_title=True, loss='loss_min_95')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
