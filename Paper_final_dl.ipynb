{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the paper will be as follows...\n",
    "\n",
    "R2 - IS and OOS, boostrap errors.\n",
    "\n",
    "New Loss function v random - IS and OOS, boostrap errors.\n",
    "\n",
    "### 0.0 Inputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from scipy.spatial.distance import pdist, squareform, jaccard, cityblock\n",
    "from scipy import stats\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import linear_model as LM\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, BayesianRidge, ElasticNet, Lasso\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Deep learning model with intermediate layer...\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=128, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=100, batch_size=5, verbose=0)))\n",
    "\n",
    "# Model dictionaries \n",
    "model_dict = {'ridge' : {'m' : Ridge, 'kw' : {'fit_intercept':True, 'alpha':0.1}, },\n",
    "              'rf'    : {'m' : RandomForestRegressor, 'kw' : {'n_estimators':100, \n",
    "                                                              'n_jobs':4, 'max_depth':10}, },\n",
    "              'svr'   : {'m' : SVR, 'kw' : {}, },\n",
    "              \n",
    "              'dl_l'   : {'m' : Pipeline, \n",
    "                          'kw' : {'steps': [('standardize', StandardScaler()),\n",
    "                                            ('mlp', KerasRegressor(build_fn=larger_model, \n",
    "                                                                   epochs=100, batch_size=5, \n",
    "                                                                   verbose=0))\n",
    "                                                           ]},\n",
    "                         },\n",
    "             }\n",
    "\n",
    "# Datasets\n",
    "from glob import glob\n",
    "targets = [s.replace('datasets/', '') for s in glob('datasets/*')]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "figsize(20, 10)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out when we want to run deep learning model\n",
    "#del model_dict['dl_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax testing...\n",
    "#del model_dict['rf']\n",
    "#del model_dict['svr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data for a single target name\n",
    "def get_data(tgt_name='COX-2'):\n",
    "    data_dir = 'datasets/' + tgt_name + '/'\n",
    "    preds = joblib.load(data_dir + tgt_name + '_predsu.npy')\n",
    "    resps = joblib.load(data_dir + tgt_name + '_respu.npy')\n",
    "    smiles = joblib.load(data_dir + tgt_name + '.smiu')\n",
    "    return preds, resps, smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def avg_mse(predictions, responses, **kwargs):\n",
    "    return mean_squared_error(responses, predictions) / mean_squared_error(responses, np.zeros_like(responses))\n",
    "\n",
    "\n",
    "def james_loss(predictions, responses, **kwargs):\n",
    "    \n",
    "    tgt_val = kwargs.get('tgt_val')\n",
    "    ranked = np.argsort(-predictions)\n",
    "    found = responses[ranked] >= tgt_val\n",
    "    # Number of actives\n",
    "    N_gamma = np.sum(found)\n",
    "    \n",
    "    # Size of test sets\n",
    "    N_test = predictions.shape[0]\n",
    "    lt = kwargs.get('loss_type')\n",
    "    #pdb.set_trace()\n",
    "    if lt == 'min':\n",
    "        # Equation (1) of the paper\n",
    "        loss = 1/(N_test - N_gamma) * np.min(np.arange(N_test)[found])\n",
    "    elif lt == 'avg':\n",
    "        # Equation (2) of the paper\n",
    "        loss = 1/N_gamma * 1/(N_test - N_gamma) * (np.sum(np.arange(N_test)[found]) - N_gamma * (N_gamma - 1)/2)\n",
    "        pass\n",
    "    \n",
    "    assert loss >= 0\n",
    "    assert loss <= 1\n",
    "    return loss\n",
    "\n",
    "\n",
    "def james_loss_avg(predictions, responses, **kwargs):\n",
    "    kwargs.update({'loss_type' :'avg'})\n",
    "    return james_loss(predictions, responses, **kwargs)\n",
    "\n",
    "def james_loss_min(predictions, responses, **kwargs):\n",
    "    kwargs.update({'loss_type' :'min'})\n",
    "    return james_loss(predictions, responses, **kwargs)\n",
    "\n",
    "\n",
    "def nbs_run(kwargs):\n",
    "    my_is = kwargs.get('is')\n",
    "    my_oos = kwargs.get('oos')\n",
    "    method = kwargs.get('method')\n",
    "    preds = kwargs.get('preds') + 0.\n",
    "    resps = kwargs.get('resps')\n",
    "    \n",
    "    mdl = model_dict[method]['m'](**model_dict[method]['kw'])\n",
    "    mdl.fit(preds[my_is], resps[my_is])\n",
    "        \n",
    "    predictions = mdl.predict(preds[my_oos])\n",
    "    \n",
    "    losses = {}\n",
    "    for (l, v) in loss_dict.iteritems():\n",
    "        \n",
    "        # Worth noting, we're looking for the to frac_find _in_the_oos_data_\n",
    "        # (not in the whole data - as otherwise we might be looking for something\n",
    "        # that isn't there)\n",
    "        if 'frac_find' in v['kw']:\n",
    "            N = len(resps[my_oos])\n",
    "            sorted_indices = np.argsort(resps[my_oos])\n",
    "            n = int(N * v['kw']['frac_find'])\n",
    "            tgt_val = resps[my_oos][sorted_indices[n]]\n",
    "            v['kw'].update({'tgt_val' : tgt_val})\n",
    "            \n",
    "        losses[l] = v['func'](predictions, resps[my_oos], **v['kw'])\n",
    "        pass\n",
    "    \n",
    "    return losses\n",
    "    \n",
    "    \n",
    "def full_bootstrap(preds, resps, method, num_runs=100, insample=False,\n",
    "                  frac_fit=1.0, num_kf=0, use_pool=True):\n",
    "\n",
    "    # losses is going to be a list of dicts, loss_type => value\n",
    "    losses = []\n",
    "    sorted_indices = np.argsort(resps)\n",
    "\n",
    "    N = len(resps)\n",
    "    M = int(N * frac_fit)\n",
    "    \n",
    "    # Choose insample values...\n",
    "    idx_list = []\n",
    "    for i in range(num_runs):\n",
    "        if num_kf > 0:\n",
    "            kf = KFold(n_splits=num_kf, shuffle=True)\n",
    "            for (tr_i, tst_i) in kf.split(np.arange(M)):\n",
    "                idcs = sorted_indices[tr_i]\n",
    "                pass\n",
    "            pass\n",
    "        else:\n",
    "            idcs_rand = np.random.choice(M, M)\n",
    "            idcs = sorted_indices[idcs_rand]\n",
    "            pass\n",
    "        idx_list.append({'is' : idcs})\n",
    "        pass\n",
    "    \n",
    "    # Add in all the other data...\n",
    "    for d in idx_list:\n",
    "        if insample:\n",
    "            oos = d['is']\n",
    "        else:\n",
    "            oos = np.delete(np.arange(N), d['is'])\n",
    "            pass\n",
    "        d.update({'oos' : oos, 'preds' : preds, \n",
    "                  'resps' : resps, 'method' : method})\n",
    "        pass\n",
    "            \n",
    "    if use_pool:\n",
    "        p = Pool(5)\n",
    "        losses = p.map(nbs_run, idx_list)\n",
    "        p.close()\n",
    "        p.join()\n",
    "    else:\n",
    "        losses = [nbs_run(x) for x in idx_list]\n",
    "        pass\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All correct\n",
      "r2 0.0\n",
      "james min 0.0\n",
      "james avg 0.0\n",
      "\n",
      "One right, one very wrong\n",
      "r2 0.315789473684\n",
      "james min 0.0\n",
      "james avg 0.5\n",
      "\n",
      "All wrong\n",
      "r2 1.15789473684\n",
      "james min 1.0\n",
      "james avg 1.0\n"
     ]
    }
   ],
   "source": [
    "# Unit tests for the loss functions...\n",
    "responses = np.arange(10)\n",
    "tgt_val = 8\n",
    "\n",
    "# get it totally right\n",
    "predictions = np.arange(10)\n",
    "\n",
    "print('All correct')\n",
    "print('r2', avg_mse(predictions, responses, tgt_val=tgt_val))\n",
    "print('james min', james_loss_min(predictions, responses, tgt_val=tgt_val))\n",
    "print('james avg', james_loss_avg(predictions, responses, tgt_val=tgt_val))\n",
    "\n",
    "# get it totally right where it counts, but maximally wrong elsewhere\n",
    "#predictions = np.asarray([0])\n",
    "\n",
    "# one totally right, one totally wrong\n",
    "predictions = np.roll(np.arange(10),-1)\n",
    "print('\\nOne right, one very wrong')\n",
    "print('r2', avg_mse(predictions, responses, tgt_val=tgt_val))\n",
    "print('james min', james_loss_min(predictions, responses, tgt_val=tgt_val))\n",
    "print('james avg', james_loss_avg(predictions, responses, tgt_val=tgt_val))\n",
    "\n",
    "\n",
    "# get everything wrong\n",
    "predictions = np.arange(10)[::-1]\n",
    "print('\\nAll wrong')\n",
    "print('r2', avg_mse(predictions, responses, tgt_val=tgt_val))\n",
    "print('james min', james_loss_min(predictions, responses, tgt_val=tgt_val))\n",
    "print('james avg', james_loss_avg(predictions, responses, tgt_val=tgt_val))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(results):\n",
    "    summary = {}\n",
    "    for l in loss_dict.keys():\n",
    "        vals = np.asarray([x[l] for x in results])\n",
    "        if len(vals) == 1:\n",
    "            summary[l] = {'loss' : vals[0]}\n",
    "        else:\n",
    "            summary[l] = {'loss_l' : np.percentile(vals, 5),\n",
    "                          'loss' : np.mean(vals),                          \n",
    "                          'loss_u' : np.percentile(vals, 95),\n",
    "                         }\n",
    "            pass\n",
    "        pass\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_targets = sorted(targets, key=lambda x: len(get_data(x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "loss_dict = {'mse' :       {'func' : avg_mse,         'kw' : {}},\n",
    "             'jmin_90' :   {'func' : james_loss_min,  'kw' : {'frac_find' : 0.9}},\n",
    "             'javg_90' :   {'func' : james_loss_avg,  'kw' : {'frac_find' : 0.9}},\n",
    "             'jmin_95' :   {'func' : james_loss_min,  'kw' : {'frac_find' : 0.95}},\n",
    "             'javg_95' :   {'func' : james_loss_avg,  'kw' : {'frac_find' : 0.95}},\n",
    "             'jmin_99' :   {'func' : james_loss_min,  'kw' : {'frac_find' : 0.99}},\n",
    "             'javg_99' :   {'func' : james_loss_avg,  'kw' : {'frac_find' : 0.99}},\n",
    "                           }\n",
    "def get_fn(frac_fit, kf=0, insample=False):\n",
    "    fn = 'loss_' + str(frac_fit)\n",
    "    if kf > 0:\n",
    "        fn += '_kf_' + str(kf)\n",
    "        pass\n",
    "    if insample:\n",
    "        fn += '_insample'\n",
    "        pass\n",
    "    return os.path.join('models_final_dl', fn)\n",
    "\n",
    "def runner(frac_fit=1.0, kf=0, insample=False, use_pool=True):\n",
    "\n",
    "    fnf = get_fn(frac_fit, kf=kf, insample=insample)\n",
    "    if os.path.isfile(fnf) and not force_rerun:\n",
    "        print('Already computed')\n",
    "        return\n",
    "\n",
    "    loss_hdr = {}\n",
    "    if insample:\n",
    "        num_runs = 1\n",
    "    elif kf > 0:\n",
    "        num_runs = int(200/kf)\n",
    "    else:\n",
    "        num_runs = 200\n",
    "    \n",
    "    for tgt in sorted_targets:\n",
    "        loss_hdr[tgt] = {}\n",
    "        print ('Doing', tgt)\n",
    "        preds, resps, _ = get_data(tgt)\n",
    "        preds = preds + 0.\n",
    "        for m in model_dict.keys():\n",
    "            res = full_bootstrap(preds, resps, m, frac_fit=frac_fit, num_kf=kf,\n",
    "                                 insample=insample, num_runs=num_runs, use_pool=use_pool,\n",
    "                                )\n",
    "            loss_hdr[tgt][m] = summarize(res)\n",
    "        pass\n",
    "    joblib.dump(loss_hdr, fnf)\n",
    "    print('Completed')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "figsize(20, 10)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_rerun = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing A2a\n",
      "Doing Dopamine\n",
      "Doing Dihydrofolate\n",
      "Doing Carbonic\n",
      "Doing ABL1\n",
      "Doing opioid\n",
      "Doing Cannabinoid\n",
      "Doing Androgen\n",
      "Doing COX-1\n",
      "Doing Monoamine\n",
      "Doing LCK\n",
      "Doing Glucocorticoid\n",
      "Doing Ephrin\n",
      "Doing Caspase\n",
      "Doing Coagulation\n",
      "Doing Estrogen\n",
      "Doing B-raf\n",
      "Doing Glycogen\n",
      "Doing Vanilloid\n",
      "Doing Aurora-A\n",
      "Doing JAK2\n",
      "Doing COX-2\n",
      "Doing Acetylcholinesterase\n",
      "Doing erbB1\n",
      "Doing HERG\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# insample run...\n",
    "runner(insample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing A2a\n",
      "Doing Dopamine\n",
      "Doing Dihydrofolate\n",
      "Doing Carbonic\n",
      "Doing ABL1\n",
      "Doing opioid\n",
      "Doing Cannabinoid\n",
      "Doing Androgen\n",
      "Doing COX-1\n",
      "Doing Monoamine\n",
      "Doing LCK\n",
      "Doing Glucocorticoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process PoolWorker-4236:\n",
      "Process PoolWorker-4240:\n",
      "Process PoolWorker-4238:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 65, in mapstar\n",
      "    return map(*args)\n",
      "  File \"<ipython-input-104-394ac3e4d1d6>\", line 50, in nbs_run\n",
      "    mdl.fit(preds[my_is], resps[my_is])\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.py\", line 250, in fit\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.py\", line 151, in fit\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 1042, in fit\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 65, in mapstar\n",
      "    return map(*args)\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "  File \"<ipython-input-104-394ac3e4d1d6>\", line 50, in nbs_run\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    outs = f(ins_batch)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2653, in __call__\n",
      "    mdl.fit(preds[my_is], resps[my_is])\n",
      "    if hasattr(get_session(), '_make_callable_from_options'):\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 190, in get_session\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.py\", line 250, in fit\n",
      "    if not getattr(v, '_keras_initialized', False):\n",
      "KeyboardInterrupt\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.py\", line 151, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 1042, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "Process PoolWorker-4237:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "    self.run()\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 65, in mapstar\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 65, in mapstar\n",
      "    return map(*args)\n",
      "  File \"<ipython-input-104-394ac3e4d1d6>\", line 50, in nbs_run\n",
      "    return map(*args)\n",
      "    mdl.fit(preds[my_is], resps[my_is])\n",
      "  File \"<ipython-input-104-394ac3e4d1d6>\", line 50, in nbs_run\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.py\", line 250, in fit\n",
      "    mdl.fit(preds[my_is], resps[my_is])\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.py\", line 250, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.py\", line 151, in fit\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.py\", line 151, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 1042, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 1042, in fit\n",
      "    outs = f(ins_batch)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2667, in __call__\n",
      "    validation_steps=validation_steps)\n",
      "    return self._legacy_call(inputs)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2647, in _legacy_call\n",
      "    session = get_session()\n",
      "    outs = f(ins_batch)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2653, in __call__\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 190, in get_session\n",
      "    if hasattr(get_session(), '_make_callable_from_options'):\n",
      "    if not getattr(v, '_keras_initialized', False):\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 190, in get_session\n",
      "KeyboardInterrupt\n",
      "    if not getattr(v, '_keras_initialized', False):\n",
      "KeyboardInterrupt\n",
      "Process PoolWorker-4239:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "    outs = f(ins_batch)\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2653, in __call__\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    if hasattr(get_session(), '_make_callable_from_options'):\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/anaconda2/lib/python2.7/multiprocessing/pool.py\", line 65, in mapstar\n",
      "    return map(*args)\n",
      "  File \"<ipython-input-104-394ac3e4d1d6>\", line 50, in nbs_run\n",
      "    mdl.fit(preds[my_is], resps[my_is])\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.py\", line 250, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.py\", line 151, in fit\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 190, in get_session\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "    if not getattr(v, '_keras_initialized', False):\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 1042, in fit\n",
      "    validation_steps=validation_steps)\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2653, in __call__\n",
      "    if hasattr(get_session(), '_make_callable_from_options'):\n",
      "  File \"/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 190, in get_session\n",
      "    if not getattr(v, '_keras_initialized', False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Full OOS run...\n",
    "runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOS run with 5-fold CV for comparison\n",
    "runner(kf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOS - max activity at 0.9\n",
    "runner(frac_fit=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOS - max activity at 0.8\n",
    "runner(frac_fit=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOS - max activity at 0.6\n",
    "runner(frac_fit=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOS - max activity at 0.4\n",
    "runner(frac_fit=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(frac_fit=1.0, kf=0, insample=False, loss='mse'):\n",
    "    \n",
    "    fnf = get_fn(frac_fit, kf=0, insample=insample)\n",
    "    \n",
    "    loss_hdr = joblib.load(fnf)\n",
    "    \n",
    "    for (i, method) in enumerate(model_dict.keys()):\n",
    "        losses = np.asarray([loss_hdr[x][method][loss]['loss'] for x in sorted_targets])\n",
    "        if insample:\n",
    "            plot(np.arange(25) + 0.05*i, losses,  label=method.upper())\n",
    "        else:\n",
    "            loss_l = np.asarray([loss_hdr[x][method][loss]['loss_l'] for x in sorted_targets])\n",
    "            loss_u = np.asarray([loss_hdr[x][method][loss]['loss_u'] for x in sorted_targets])\n",
    "            \n",
    "            yerr = np.vstack((losses - loss_l, loss_u - losses))\n",
    "            \n",
    "            errorbar(np.arange(25)+i*0.1, losses, capsize=10, yerr=yerr, label=method.upper())\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    if insample:\n",
    "        title('Insample %s Loss' % loss.upper())\n",
    "    else:\n",
    "        ttl = 'OOS %s Loss' % loss.upper()\n",
    "        if kf > 0:\n",
    "            ttl += ' with %d fold CV' % kf\n",
    "        if frac_fit < 1.0:\n",
    "            ttl += ' Max activity in fit at %.1f' % frac_fit\n",
    "        title(ttl)\n",
    "    pass\n",
    "\n",
    "    grid(True)\n",
    "    plt.xticks(np.arange(25), sorted_targets, rotation=-45)\n",
    "    legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insample loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(insample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(insample=True, loss='jmin_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(insample=True, loss='jmin_99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(insample=True, loss='javg_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOS (no fit restriction) loss plots (using CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(kf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(kf=5, loss='javg_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOS (no fit restriction) loss plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(loss='javg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(loss='jmin_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOS plots (0.9 frac fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.9, loss='javg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.9, loss='jmin_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOS loss (max activity at 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.8, loss='javg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.8, loss='jmin_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOS loss - max activity 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.6, loss='javg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.6, loss='jmin_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOS - max activity in fit 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.4, loss='javg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.4, loss='jmin_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(frac_fit=0.4, loss='jmin_99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LL estimates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the estimates have wide tails away from 0, we're going to (as a conservative estimate)\n",
    "# of relative log likelihood, look at how many (2-lower-standard errors away from 0 the mean is)\n",
    "\n",
    "def ll_estimate(frac_fit=1.0, losses=['javg_90', 'mse'], kf=0):\n",
    "    fnf = get_fn(frac_fit, kf=0)\n",
    "    loss_hdr = joblib.load(fnf)\n",
    "    \n",
    "    print(('%9s |' + '%9s |' * len(losses)) % tuple(['',] + losses))\n",
    "    \n",
    "    print('-' * (11 * (len(losses) + 1) - 1))\n",
    "    for (i, method) in enumerate(model_dict.keys()):\n",
    "        \n",
    "        lls = []\n",
    "        \n",
    "        for loss in losses:\n",
    "            ll = 0\n",
    "            for tgt in sorted_targets:\n",
    "                lmean = loss_hdr[tgt][method][loss]['loss']\n",
    "                llow = loss_hdr[tgt][method][loss]['loss_l']\n",
    "                sigmas = (lmean - llow)/2\n",
    "                if lmean == 0:\n",
    "                    continue # same as subrtacting 0\n",
    "                else:\n",
    "                    sigmas_from_zero = lmean/sigmas\n",
    "                    ll += sigmas_from_zero**2/2\n",
    "                    pass\n",
    "                pass\n",
    "            lls.append(ll)\n",
    "            pass\n",
    "        \n",
    "        print (('%9s |'+ '%9.1f |' * len(losses)) % tuple([method,] + lls))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate(frac_fit=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate(frac_fit=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate(frac_fit=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate(frac_fit=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate(losses=loss_dict.keys(), frac_fit=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate(losses=loss_dict.keys(), frac_fit=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate(losses=loss_dict.keys(), frac_fit=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_estimate(losses=loss_dict.keys(), frac_fit=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
