{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the analysis is as follows...\n",
    "\n",
    "- We define up two rank based loss functions: these are functions of the predicted ranks of the 'active' molecules. The first only considers the rank of the highest ranked active, the second considers the average of all the ranks of the actives.\n",
    "- We do standard 5-fold cross-validation and bootstrap cross-validation. Out of sample predictions are evaluated using the above active-rank loss functions and mean squared error.\n",
    "- We progressively lower the threshold activity (in terms of the quantile of the activity distribution) and fit the models on a boostrapped datasets with only molecules that have activity beneath this threshold.\n",
    "- We look at additive loss functions on which to train (penalize) the models better reflecting the out of sample loss.\n",
    "\n",
    "\n",
    "## Inputs and main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from scipy.spatial.distance import pdist, squareform, jaccard, cityblock\n",
    "from scipy import stats\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import linear_model as LM\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, BayesianRidge, ElasticNet, Lasso\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Deep learning model with intermediate layer...\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=128, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=100, batch_size=5, verbose=0)))\n",
    "\n",
    "# Model dictionaries \n",
    "model_dict = {'ridge' : {'m' : Ridge, 'kw' : {'fit_intercept':True, 'alpha':0.1}, },\n",
    "              'rf'    : {'m' : RandomForestRegressor, 'kw' : {'n_estimators':100, \n",
    "                                                              'n_jobs':4, 'max_depth':10}, },\n",
    "              'svr'   : {'m' : SVR, 'kw' : {}, },\n",
    "              \n",
    "              'dl_l'   : {'m' : Pipeline, \n",
    "                          'kw' : {'steps': [('standardize', StandardScaler()),\n",
    "                                            ('mlp', KerasRegressor(build_fn=larger_model, \n",
    "                                                                   epochs=100, batch_size=5, \n",
    "                                                                   verbose=0))\n",
    "                                                           ]},\n",
    "                         },\n",
    "             }\n",
    "\n",
    "# Datasets\n",
    "from glob import glob\n",
    "targets = [s.replace('datasets/', '') for s in glob('datasets/*')]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "figsize(20, 10)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs without deep learning are much faster - this controls use of DL\n",
    "do_dl = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for overall number of runs\n",
    "if not do_dl:\n",
    "    # Take out when we want to run deep learning model\n",
    "    del model_dict['dl_l']\n",
    "    tot_num_runs = 400\n",
    "    outdir = 'models_final/'\n",
    "    fig_dir = 'figures/'\n",
    "    ['svr', 'rf', 'ridge']\n",
    "    model_labels = ['Support Vector Regression','Random Forests','Ridge Regression']\n",
    "else:\n",
    "    tot_num_runs = 100\n",
    "    outdir = 'models_final_dl/'\n",
    "    fig_dir = 'figures_dl/'\n",
    "    model_labels = ['Support Vector Regression','Random Forests','Ridge Regression','Deep Learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in data for a single target name\n",
    "def get_data(tgt_name='COX-2'):\n",
    "    data_dir = 'datasets/' + tgt_name + '/'\n",
    "    preds = joblib.load(data_dir + tgt_name + '_predsu.npy')\n",
    "    resps = joblib.load(data_dir + tgt_name + '_respu.npy')\n",
    "    smiles = joblib.load(data_dir + tgt_name + '.smiu')\n",
    "    return preds, resps, smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the loss functions \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def avg_mse(predictions, responses, **kwargs):\n",
    "    return mean_squared_error(responses, predictions) / mean_squared_error(responses, np.zeros_like(responses))\n",
    "\n",
    "\n",
    "def Rank_loss(predictions, responses, **kwargs):\n",
    "    \n",
    "    tgt_val = kwargs.get('tgt_val')\n",
    "    ranked = np.argsort(-predictions)\n",
    "    found = responses[ranked] >= tgt_val\n",
    "    # Number of actives\n",
    "    N_gamma = np.sum(found)\n",
    "    \n",
    "    # Size of test sets\n",
    "    N_test = predictions.shape[0]\n",
    "    lt = kwargs.get('loss_type')\n",
    "    #pdb.set_trace()\n",
    "    if lt == 'min':\n",
    "        # Equation (1) of the paper\n",
    "        loss = 1/(N_test - N_gamma) * np.min(np.arange(N_test)[found])\n",
    "    elif lt == 'avg':\n",
    "        # Equation (2) of the paper\n",
    "        loss = 1/N_gamma * 1/(N_test - N_gamma) * (np.sum(np.arange(N_test)[found]) - N_gamma * (N_gamma - 1)/2)\n",
    "        pass\n",
    "    \n",
    "    assert loss >= 0\n",
    "    assert loss <= 1\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Active_rank_loss_avg(predictions, responses, **kwargs):\n",
    "    kwargs.update({'loss_type' :'avg'})\n",
    "    return Rank_loss(predictions, responses, **kwargs)\n",
    "\n",
    "def Active_rank_loss_min(predictions, responses, **kwargs):\n",
    "    kwargs.update({'loss_type' :'min'})\n",
    "    return Rank_loss(predictions, responses, **kwargs)\n",
    "\n",
    "\n",
    "def nbs_run(kwargs):\n",
    "    my_is = kwargs.get('is')\n",
    "    my_oos = kwargs.get('oos')\n",
    "    method = kwargs.get('method')\n",
    "    preds = kwargs.get('preds') + 0.\n",
    "    resps = kwargs.get('resps')\n",
    "    \n",
    "    mdl = model_dict[method]['m'](**model_dict[method]['kw'])\n",
    "    mdl.fit(preds[my_is], resps[my_is])\n",
    "        \n",
    "    predictions = mdl.predict(preds[my_oos])\n",
    "    \n",
    "    losses = {}\n",
    "    for (l, v) in loss_dict.iteritems():\n",
    "        \n",
    "        # Worth noting, we're looking for the top frac_find _in_the_oos_data_\n",
    "        # (not in the whole data - as otherwise we might be looking for something\n",
    "        # that isn't there)\n",
    "        if 'frac_find' in v['kw']:\n",
    "            N = len(resps[my_oos])\n",
    "            sorted_indices = np.argsort(resps[my_oos])\n",
    "            n = int(N * v['kw']['frac_find'])\n",
    "            tgt_val = resps[my_oos][sorted_indices[n]]\n",
    "            v['kw'].update({'tgt_val' : tgt_val})\n",
    "            \n",
    "        losses[l] = v['func'](predictions, resps[my_oos], **v['kw'])\n",
    "        pass\n",
    "    \n",
    "    return losses\n",
    "    \n",
    "    \n",
    "def full_bootstrap(preds, resps, method, num_runs=100, insample=False,\n",
    "                  frac_fit=1.0, num_kf=0, use_pool=True):\n",
    "\n",
    "    # losses is going to be a list of dicts, loss_type => value\n",
    "    losses = []\n",
    "    sorted_indices = np.argsort(resps)\n",
    "\n",
    "    N = len(resps)\n",
    "    M = int(N * frac_fit)\n",
    "    \n",
    "    # Choose insample values...\n",
    "    idx_list = []\n",
    "    for i in range(num_runs):\n",
    "        if num_kf > 0:\n",
    "            kf = KFold(n_splits=num_kf, shuffle=True)\n",
    "            for (tr_i, tst_i) in kf.split(np.arange(M)):\n",
    "                idcs = sorted_indices[tr_i]\n",
    "                pass\n",
    "            pass\n",
    "        else:\n",
    "            idcs_rand = np.random.choice(M, M)\n",
    "            idcs = sorted_indices[idcs_rand]\n",
    "            pass\n",
    "        idx_list.append({'is' : idcs})\n",
    "        pass\n",
    "    \n",
    "    # Add in all the other data...\n",
    "    for d in idx_list:\n",
    "        if insample:\n",
    "            oos = d['is']\n",
    "        else:\n",
    "            oos = np.delete(np.arange(N), d['is'])\n",
    "            pass\n",
    "        d.update({'oos' : oos, 'preds' : preds, \n",
    "                  'resps' : resps, 'method' : method})\n",
    "        pass\n",
    "            \n",
    "    if use_pool:\n",
    "        p = Pool(7)\n",
    "        losses = p.map(nbs_run, idx_list)\n",
    "        p.close()\n",
    "        p.join()\n",
    "    else:\n",
    "        losses = [nbs_run(x) for x in idx_list]\n",
    "        pass\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All correct\n",
      "r2 0.0\n",
      "Active rank min 0.0\n",
      "Active rank avg 0.0\n",
      "\n",
      "One right, one very wrong\n",
      "r2 0.315789473684\n",
      "Active rank min 0.0\n",
      "Active rank avg 0.5\n",
      "\n",
      "All wrong\n",
      "r2 1.15789473684\n",
      "Active rank min 1.0\n",
      "Active rank avg 1.0\n"
     ]
    }
   ],
   "source": [
    "# Unit tests for the loss functions...\n",
    "responses = np.arange(10)\n",
    "tgt_val = 8\n",
    "\n",
    "# get it totally right\n",
    "predictions = np.arange(10)\n",
    "\n",
    "print('All correct')\n",
    "print('r2', avg_mse(predictions, responses, tgt_val=tgt_val))\n",
    "print('Active rank min', Active_rank_loss_min(predictions, responses, tgt_val=tgt_val))\n",
    "print('Active rank avg', Active_rank_loss_avg(predictions, responses, tgt_val=tgt_val))\n",
    "\n",
    "# get it totally right where it counts, but maximally wrong elsewhere\n",
    "#predictions = np.asarray([0])\n",
    "\n",
    "# one totally right, one totally wrong\n",
    "predictions = np.roll(np.arange(10),-1)\n",
    "print('\\nOne right, one very wrong')\n",
    "print('r2', avg_mse(predictions, responses, tgt_val=tgt_val))\n",
    "print('Active rank min', Active_rank_loss_min(predictions, responses, tgt_val=tgt_val))\n",
    "print('Active rank avg', Active_rank_loss_avg(predictions, responses, tgt_val=tgt_val))\n",
    "\n",
    "\n",
    "# get everything wrong\n",
    "predictions = np.arange(10)[::-1]\n",
    "print('\\nAll wrong')\n",
    "print('r2', avg_mse(predictions, responses, tgt_val=tgt_val))\n",
    "print('Active rank min', Active_rank_loss_min(predictions, responses, tgt_val=tgt_val))\n",
    "print('Active rank avg', Active_rank_loss_avg(predictions, responses, tgt_val=tgt_val))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(results):\n",
    "    summary = {}\n",
    "    for l in loss_dict.keys():\n",
    "        vals = np.asarray([x[l] for x in results])\n",
    "        if len(vals) == 1:\n",
    "            summary[l] = {'loss' : vals[0]}\n",
    "        else:\n",
    "            summary[l] = {'loss_l' : np.percentile(vals, 5),\n",
    "                          'loss' : np.mean(vals),                          \n",
    "                          'loss_u' : np.percentile(vals, 95),\n",
    "                         }\n",
    "            pass\n",
    "        pass\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the vector of observed values and their mean (implies fewer computations of the mean)\n",
    "def jackknife(vals, vals_bar):\n",
    "    n = len(vals)\n",
    "    var_JK = 0.0\n",
    "    for v in vals:\n",
    "        vals_bar_i = (n/(n-1)) * (vals_bar - v/n)\n",
    "        #print(pow(vals_bar_i - vals_bar, 2))\n",
    "        var_JK += pow(vals_bar_i - vals_bar, 2)\n",
    "        pass\n",
    "\n",
    "    sd_JK = pow( ((n-1)/n) * var_JK, .5)\n",
    "   \n",
    "    return sd_JK\n",
    "    \n",
    "def jackknife_summary(results):\n",
    "    summary = {}\n",
    "    for l in loss_dict.keys():\n",
    "        vals = np.asarray([x[l] for x in results])\n",
    "        if len(vals) == 1:\n",
    "            summary[l] = {'loss' : vals[0]}\n",
    "        else:\n",
    "            vals_bar = np.mean(vals)\n",
    "            sd = jackknife(vals, vals_bar)\n",
    "            summary[l] = {'loss_l' : vals_bar - 2*sd,\n",
    "                          'loss' : vals_bar,                          \n",
    "                          'loss_u' : vals_bar + 2*sd,\n",
    "                         }\n",
    "            pass\n",
    "        pass\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_targets = sorted(targets, key=lambda x: len(get_data(x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iuhidijgbdbs\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "gg = 'iuhid.ijgbdbs'\n",
    "gg = re.sub('[.]','', gg)\n",
    "print(gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "\n",
    "# define the dictionary of losses used here\n",
    "loss_dict = {'mse' :       {'func' : avg_mse,         'kw' : {}},\n",
    "             'loss_min_90' :   {'func' : Active_rank_loss_min,  'kw' : {'frac_find' : 0.9}},\n",
    "             'loss_avg_90' :   {'func' : Active_rank_loss_avg,  'kw' : {'frac_find' : 0.9}},\n",
    "             'loss_min_95' :   {'func' : Active_rank_loss_min,  'kw' : {'frac_find' : 0.95}},\n",
    "             'loss_avg_95' :   {'func' : Active_rank_loss_avg,  'kw' : {'frac_find' : 0.95}},\n",
    "             'loss_min_99' :   {'func' : Active_rank_loss_min,  'kw' : {'frac_find' : 0.99}},\n",
    "             'loss_avg_99' :   {'func' : Active_rank_loss_avg,  'kw' : {'frac_find' : 0.99}},\n",
    "                           }\n",
    "def get_fn(frac_fit, kf=0, insample=False, fig=False, loss=None):\n",
    "    fn = 'loss_' + str(frac_fit)\n",
    "    if kf > 0:\n",
    "        fn += '_kf_' + str(kf)\n",
    "        pass\n",
    "    if insample:\n",
    "        fn += '_insample'\n",
    "        pass\n",
    "    od = fig_dir if fig else outdir\n",
    "    ffn = os.path.join(od, fn)\n",
    "    if fig:\n",
    "        ffn = re.sub('[.]','', ffn)\n",
    "        ffn = ffn + '_' + loss + '.pdf'\n",
    "        pass\n",
    "    return ffn\n",
    "        \n",
    "\n",
    "def runner(frac_fit=1.0, kf=0, insample=False, use_pool=True):\n",
    "\n",
    "    fnf = get_fn(frac_fit, kf=kf, insample=insample)\n",
    "    if os.path.isfile(fnf) and not force_rerun:\n",
    "        print('Already computed')\n",
    "        return\n",
    "\n",
    "    loss_hdr = {}\n",
    "    if insample:\n",
    "        num_runs = 1\n",
    "    elif kf > 0:\n",
    "        num_runs = int(tot_num_runs/kf)\n",
    "    else:\n",
    "        num_runs = tot_num_runs\n",
    "    \n",
    "    for tgt in sorted_targets:\n",
    "        loss_hdr[tgt] = {}\n",
    "        print ('Doing', tgt)\n",
    "        preds, resps, _ = get_data(tgt)\n",
    "        preds = preds + 0.\n",
    "        for m in model_dict.keys():\n",
    "            res = full_bootstrap(preds, resps, m, frac_fit=frac_fit, num_kf=kf,\n",
    "                                 insample=insample, num_runs=num_runs, use_pool=use_pool,\n",
    "                                )\n",
    "            joblib.dump(res, outdir + '/detail/' + tgt + '_' + m + '_' + str(frac_fit) + '_' + str(kf) + '.res')\n",
    "            loss_hdr[tgt][m] = jackknife_summary(res)\n",
    "        pass\n",
    "    joblib.dump(loss_hdr, fnf)\n",
    "    print('Completed')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "figsize(20, 10)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_rerun = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed\n"
     ]
    }
   ],
   "source": [
    "# insample run...\n",
    "runner(insample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed\n"
     ]
    }
   ],
   "source": [
    "# Full OOS run...\n",
    "runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed\n"
     ]
    }
   ],
   "source": [
    "# OOS run with 5-fold CV for comparison\n",
    "runner(kf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed\n"
     ]
    }
   ],
   "source": [
    "# OOS - max activity at 0.9\n",
    "runner(frac_fit=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed\n"
     ]
    }
   ],
   "source": [
    "# OOS - max activity at 0.8\n",
    "runner(frac_fit=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed\n"
     ]
    }
   ],
   "source": [
    "# OOS - max activity at 0.6\n",
    "runner(frac_fit=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed\n"
     ]
    }
   ],
   "source": [
    "# OOS - max activity at 0.4\n",
    "runner(frac_fit=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svr', 'rf', 'ridge']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots results\n",
    "# Added put_title arg for plots for paper\n",
    "def plotter(frac_fit=1.0, kf=0, insample=False, loss='mse', save=True, put_title=False, put_grid=False):\n",
    "    \n",
    "    fnf = get_fn(frac_fit, kf=kf, insample=insample)\n",
    "    \n",
    "    loss_hdr = joblib.load(fnf)\n",
    "   \n",
    "    for (i, method) in enumerate(model_dict.keys()):\n",
    "        losses = np.asarray([loss_hdr[x][method][loss]['loss'] for x in sorted_targets])\n",
    "        if insample:\n",
    "            plot(np.arange(25) + 0.05*i, losses,  label=method.upper())\n",
    "        else:\n",
    "            loss_l = np.asarray([loss_hdr[x][method][loss]['loss_l'] for x in sorted_targets])\n",
    "            loss_u = np.asarray([loss_hdr[x][method][loss]['loss_u'] for x in sorted_targets])\n",
    "            \n",
    "            yerr = np.vstack((losses - loss_l, loss_u - losses))\n",
    "            \n",
    "            errorbar(np.arange(25)+i*0.1, losses, capsize=10, yerr=yerr, label=method.upper())\n",
    "            pass\n",
    "        pass\n",
    "        \n",
    "    if insample:\n",
    "        if put_title:\n",
    "            title('Insample %s Loss' % loss.upper())\n",
    "    else:\n",
    "        ttl = 'OOS %s Loss' % loss.upper()\n",
    "        if kf > 0:\n",
    "            ttl += ' with %d fold CV' % kf\n",
    "        if frac_fit < 1.0:\n",
    "            ttl += ' Max activity in fit at %.1f' % frac_fit\n",
    "        if put_title:\n",
    "            title(ttl)\n",
    "    pass\n",
    "\n",
    "    grid(put_grid)\n",
    "    plt.tick_params(top=False, right=False)\n",
    "    plt.xticks(np.arange(25), sorted_targets, rotation=-45)\n",
    "    plt.legend(loc='best', fontsize = 'x-large', labels=model_labels)\n",
    "    if loss=='mse':\n",
    "        plt.ylabel('Expected mean squared error', fontsize='x-large')\n",
    "    else:\n",
    "        plt.ylabel('Expected loss', fontsize='x-large')\n",
    "    \n",
    "    if save:\n",
    "        fnff = get_fn(frac_fit, kf=kf, insample=insample, fig=True, loss=loss)\n",
    "        print(fnff)\n",
    "        savefig(fnff, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figures/loss_10_insample_mse.pdf\n"
     ]
    }
   ],
   "source": [
    "# We plot the in-sample MSE loss\n",
    "plotter(insample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the in-sample active-rank min loss, with gamma = 0.9\n",
    "plotter(insample=True, loss='loss_min_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the in-sample active-rank min loss, with gamma = 0.99\n",
    "plotter(insample=True, loss='loss_min_99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the in-sample active-rank average loss, with gamma = 0.9\n",
    "plotter(insample=True, loss='loss_avg_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample (random partitioning) loss plots (using CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold CV, scored by MSE\n",
    "plotter(kf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold CV, scored by active-rank average loss, gamma = 0.9\n",
    "plotter(kf=5, loss='loss_avg_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample (random partitioning) loss plots (bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap CV, scored by MSE\n",
    "plotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap CV, scored by active-rank average loss, gamma = 0.9\n",
    "plotter(loss='loss_avg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap CV, scored by active-rank min loss, gamma = 0.9\n",
    "plotter(loss='loss_min_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample loss plots ( training on thresholded data: q=0.9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.9, scored by MSE\n",
    "plotter(frac_fit=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.9, scored by active-rank average loss, gamma = 0.9\n",
    "plotter(frac_fit=0.9, loss='loss_avg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.9, scored by active-rank min loss, gamma = 0.9\n",
    "plotter(frac_fit=0.9, loss='loss_min_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample loss plots ( training on thresholded data: q=0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.8, scored by MSE\n",
    "plotter(frac_fit=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.8, scored by active-rank average loss, gamma = 0.9\n",
    "plotter(frac_fit=0.8, loss='loss_avg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.8, scored by active-rank min loss, gamma = 0.9\n",
    "plotter(frac_fit=0.8, loss='loss_min_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample loss plots ( training on thresholded data: q=0.6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.6, scored by MSE\n",
    "plotter(frac_fit=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.6, scored by active-rank average loss, gamma = 0.9\n",
    "plotter(frac_fit=0.6, loss='loss_avg_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.6, scored by active-rank min loss, gamma = 0.9\n",
    "plotter(frac_fit=0.6, loss='loss_min_90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample loss plots ( training on thresholded data: q=0.4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.4, scored by MSE\n",
    "plotter(frac_fit=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.4, scored by active-rank average loss, gamma = 0.9\n",
    "plotter(frac_fit=0.4, loss='loss_avg_99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.4, scored by active-rank min loss, gamma = 0.9\n",
    "plotter(frac_fit=0.4, loss='loss_min_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set activity thresholded at q=0.4, scored by active-rank min loss, gamma = 0.99\n",
    "plotter(frac_fit=0.4, loss='loss_min_99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_min(means, sigmas, K=1000, epsilon = 0.01):\n",
    "    \n",
    "    N = len(means)\n",
    "    probs = np.zeros(N) # array of probability weights\n",
    "    \n",
    "    # check that none of the sigmas are zero to avoid NANs\n",
    "    for ss in range(N):\n",
    "        if(sigmas[ss]==0):\n",
    "            sigmas[ss] = epsilon\n",
    "            pass\n",
    "        pass\n",
    "\n",
    "    for i in range(N):\n",
    "        xs = np.random.normal(loc=means[i],scale=sigmas[i],size=K)\n",
    "        logterms = np.zeros(K)\n",
    "        # Iterate over random draw from the i^th normal distribution\n",
    "        for sim in range(K):\n",
    "            x = xs[sim]\n",
    "            logproductterm = 0\n",
    "            for j in range(N):\n",
    "                if(j != i):\n",
    "                    logproductterm += stats.norm.logcdf(-(x-means[j])/sigmas[j]) \n",
    "                    pass\n",
    "                pass\n",
    "            logterms[sim] = logproductterm\n",
    "            pass\n",
    "        probs[i] = np.mean(np.exp(logterms))\n",
    "        pass\n",
    "    return probs\n",
    "\n",
    "# Total score : this is change of ll_estimate\n",
    "def model_score(frac_fit=1.0, losses=['loss_avg_90', 'mse', ], kf=0, K=1000, recompute_scores=False):\n",
    "    if recompute_scores:\n",
    "        fnf = get_fn(frac_fit, kf=0)\n",
    "        loss_hdr = joblib.load(fnf)\n",
    "\n",
    "        print(('%9s |' + '%9s |' * len(losses)) % tuple(['',] + losses))\n",
    "        print('-' * (11 * (len(losses) + 1) - 1))\n",
    "\n",
    "        M = len(model_dict.keys()) # number of models\n",
    "\n",
    "        Loss_scores = []\n",
    "\n",
    "        models = model_dict.keys()\n",
    "        d = {'losses' : map(lambda x : x + '_' + str(frac_fit), losses)}\n",
    "\n",
    "        for loss in losses:\n",
    "            scores = np.zeros(M)\n",
    "            for tgt in sorted_targets:\n",
    "                tgt_means = []\n",
    "                tgt_sigmas = []\n",
    "                # For each model we extract the mean and SD\n",
    "                for (i, method) in enumerate(models):\n",
    "                    lmean = loss_hdr[tgt][method][loss]['loss']\n",
    "                    llow = loss_hdr[tgt][method][loss]['loss_l']\n",
    "                    lsigma = (lmean - llow)/2\n",
    "                    tgt_means.append(lmean)\n",
    "                    tgt_sigmas.append(lsigma)\n",
    "                    pass\n",
    "                # The arrays of means and SDs are used to compute probability of min expected loss\n",
    "                weights = probability_min(tgt_means, tgt_sigmas, K=K)\n",
    "                # A weight of 1: min loss with probability 1; weight of 0: min loss with probability 0\n",
    "\n",
    "                scores += weights   \n",
    "                pass\n",
    "            Loss_scores.append(scores)\n",
    "\n",
    "\n",
    "\n",
    "        for (i, method) in enumerate(models):\n",
    "            d[method] = np.ravel(map(lambda x: x[i], Loss_scores))\n",
    "            out = [method,]\n",
    "            for (j, loss) in enumerate(losses):\n",
    "                out.append(Loss_scores[j][i])\n",
    "                pass\n",
    "            print (('%9s |' + '%9.1f |' * len(losses)) % tuple(out))\n",
    "\n",
    "        return d\n",
    "    else:\n",
    "        print('done this already, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this when the models are rerun\n",
    "recompute_scores = False\n",
    "\n",
    "d_oos = model_score(losses=loss_dict.keys(), recompute_scores=recompute_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_09 = model_score(losses=loss_dict.keys(), frac_fit=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_08 = model_score(losses=loss_dict.keys(), frac_fit=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_06 = model_score(losses=loss_dict.keys(), frac_fit=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_04 = model_score(losses=loss_dict.keys(), frac_fit=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recompute_scores:\n",
    "    pd0 = pd.DataFrame(d_oos)\n",
    "    for d in (d_09, d_08, d_06, d_04):\n",
    "        pdd = pd.DataFrame(d)\n",
    "        pd0 = pd0.append(pdd, ignore_index=True)\n",
    "        pass\n",
    "    pd0\n",
    "    if do_dl:\n",
    "        pd0.to_csv('models_final_dl/loss_summary.csv')\n",
    "    else:\n",
    "        pd0.to_csv('models_final/loss_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
